{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686b2956",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES\n",
    " we are using tensorflow for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15307df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dca696cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf19790",
   "metadata": {},
   "source": [
    "## LOADING DATASET (iris datase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1331cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945abfa",
   "metadata": {},
   "source": [
    "### preprocessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41408b",
   "metadata": {},
   "source": [
    "using pandas to understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "484f9e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    35\n",
       "sepal width (cm)     23\n",
       "petal length (cm)    43\n",
       "petal width (cm)     22\n",
       "species_target        3\n",
       "species_name          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 2. Create the DataFrame using the data and feature names\n",
    "iris_df = pd.DataFrame(data=x, columns=iris.feature_names)\n",
    "\n",
    "# 3. Add the target variable (species) to the DataFrame\n",
    "iris_df['species_target'] = y\n",
    "iris_df['species_name'] = iris.target_names[y]\n",
    "\n",
    "iris_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9e0af",
   "metadata": {},
   "source": [
    "there is : 4 inputs , 3 outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e647b4",
   "metadata": {},
   "source": [
    "and doing stuff to stop the model from overfitting (belive me it was overfitting super bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5662f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into testing and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4,random_state=42)\n",
    "x_test,x_val,y_test,y_val = train_test_split(x_test,y_test,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8da8b",
   "metadata": {},
   "source": [
    "scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44709c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77db5c5",
   "metadata": {},
   "source": [
    "## MAKING THE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7abcc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model with regularization and dropout\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(4,)),\n",
    "    layers.Dense(8, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(4, activation='relu', kernel_regularizer='l2'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#compiling model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d17ca",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17bc3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.2889 - loss: 1.2338 - val_accuracy: 0.4000 - val_loss: 1.1595\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3556 - loss: 1.2219 - val_accuracy: 0.5667 - val_loss: 1.1325\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4222 - loss: 1.1900 - val_accuracy: 0.7000 - val_loss: 1.1141\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4111 - loss: 1.1816 - val_accuracy: 0.7333 - val_loss: 1.0986\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4444 - loss: 1.1554 - val_accuracy: 0.7333 - val_loss: 1.0851\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4111 - loss: 1.1674 - val_accuracy: 0.7000 - val_loss: 1.0694\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5222 - loss: 1.1160 - val_accuracy: 0.6667 - val_loss: 1.0544\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4222 - loss: 1.1415 - val_accuracy: 0.7000 - val_loss: 1.0395\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4667 - loss: 1.1360 - val_accuracy: 0.7000 - val_loss: 1.0244\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4889 - loss: 1.1025 - val_accuracy: 0.7000 - val_loss: 1.0056\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5333 - loss: 1.0801 - val_accuracy: 0.7000 - val_loss: 0.9865\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5222 - loss: 1.0851 - val_accuracy: 0.7000 - val_loss: 0.9666\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5667 - loss: 1.0634 - val_accuracy: 0.7000 - val_loss: 0.9477\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6556 - loss: 1.0356 - val_accuracy: 0.6667 - val_loss: 0.9308\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6222 - loss: 0.9937 - val_accuracy: 0.6667 - val_loss: 0.9121\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5778 - loss: 1.0035 - val_accuracy: 0.6667 - val_loss: 0.8926\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6000 - loss: 0.9993 - val_accuracy: 0.6667 - val_loss: 0.8753\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5444 - loss: 1.0001 - val_accuracy: 0.6667 - val_loss: 0.8597\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5667 - loss: 0.9999 - val_accuracy: 0.6667 - val_loss: 0.8464\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6000 - loss: 0.9649 - val_accuracy: 0.6667 - val_loss: 0.8324\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7111 - loss: 0.9123 - val_accuracy: 0.6667 - val_loss: 0.8192\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6111 - loss: 0.9370 - val_accuracy: 0.6667 - val_loss: 0.8049\n",
      "Epoch 23/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5778 - loss: 0.9127 - val_accuracy: 0.6667 - val_loss: 0.7889\n",
      "Epoch 24/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.8950 - val_accuracy: 0.6667 - val_loss: 0.7756\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7111 - loss: 0.8824 - val_accuracy: 0.6667 - val_loss: 0.7631\n",
      "Epoch 26/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6333 - loss: 0.9238 - val_accuracy: 0.6667 - val_loss: 0.7529\n",
      "Epoch 27/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7667 - loss: 0.8625 - val_accuracy: 0.6667 - val_loss: 0.7437\n",
      "Epoch 28/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.8326 - val_accuracy: 0.6667 - val_loss: 0.7342\n",
      "Epoch 29/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6556 - loss: 0.8787 - val_accuracy: 0.6667 - val_loss: 0.7243\n",
      "Epoch 30/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7000 - loss: 0.8537 - val_accuracy: 0.7000 - val_loss: 0.7142\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6556 - loss: 0.8674 - val_accuracy: 0.7000 - val_loss: 0.7061\n",
      "Epoch 32/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6222 - loss: 0.8959 - val_accuracy: 0.7000 - val_loss: 0.6984\n",
      "Epoch 33/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.8513 - val_accuracy: 0.7000 - val_loss: 0.6907\n",
      "Epoch 34/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.8486 - val_accuracy: 0.7000 - val_loss: 0.6831\n",
      "Epoch 35/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6333 - loss: 0.8281 - val_accuracy: 0.7000 - val_loss: 0.6756\n",
      "Epoch 36/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7000 - loss: 0.8058 - val_accuracy: 0.7000 - val_loss: 0.6692\n",
      "Epoch 37/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6667 - loss: 0.8044 - val_accuracy: 0.7000 - val_loss: 0.6630\n",
      "Epoch 38/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.7973 - val_accuracy: 0.7000 - val_loss: 0.6558\n",
      "Epoch 39/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7222 - loss: 0.7712 - val_accuracy: 0.7000 - val_loss: 0.6503\n",
      "Epoch 40/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6222 - loss: 0.8317 - val_accuracy: 0.7000 - val_loss: 0.6457\n",
      "Epoch 41/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6444 - loss: 0.8393 - val_accuracy: 0.7000 - val_loss: 0.6400\n",
      "Epoch 42/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6889 - loss: 0.7857 - val_accuracy: 0.7000 - val_loss: 0.6355\n",
      "Epoch 43/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6667 - loss: 0.8200 - val_accuracy: 0.7000 - val_loss: 0.6301\n",
      "Epoch 44/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.7795 - val_accuracy: 0.7000 - val_loss: 0.6268\n",
      "Epoch 45/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.7665 - val_accuracy: 0.7000 - val_loss: 0.6235\n",
      "Epoch 46/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7889 - loss: 0.7367 - val_accuracy: 0.7000 - val_loss: 0.6195\n",
      "Epoch 47/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6667 - loss: 0.7991 - val_accuracy: 0.7000 - val_loss: 0.6162\n",
      "Epoch 48/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7556 - loss: 0.7794 - val_accuracy: 0.7000 - val_loss: 0.6128\n",
      "Epoch 49/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7222 - loss: 0.7963 - val_accuracy: 0.7000 - val_loss: 0.6076\n",
      "Epoch 50/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6889 - loss: 0.7599 - val_accuracy: 0.7000 - val_loss: 0.6049\n",
      "Epoch 51/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7000 - loss: 0.7515 - val_accuracy: 0.7000 - val_loss: 0.6019\n",
      "Epoch 52/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7111 - loss: 0.7469 - val_accuracy: 0.7000 - val_loss: 0.5985\n",
      "Epoch 53/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7444 - loss: 0.7355 - val_accuracy: 0.7000 - val_loss: 0.5952\n",
      "Epoch 54/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.7468 - val_accuracy: 0.7000 - val_loss: 0.5922\n",
      "Epoch 55/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6889 - loss: 0.7563 - val_accuracy: 0.7000 - val_loss: 0.5900\n",
      "Epoch 56/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7444 - loss: 0.7360 - val_accuracy: 0.7000 - val_loss: 0.5854\n",
      "Epoch 57/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6556 - loss: 0.8049 - val_accuracy: 0.7000 - val_loss: 0.5818\n",
      "Epoch 58/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.6857 - val_accuracy: 0.7000 - val_loss: 0.5786\n",
      "Epoch 59/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.6813 - val_accuracy: 0.7000 - val_loss: 0.5760\n",
      "Epoch 60/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.7288 - val_accuracy: 0.7333 - val_loss: 0.5736\n",
      "Epoch 61/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.7025 - val_accuracy: 0.7333 - val_loss: 0.5717\n",
      "Epoch 62/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6889 - loss: 0.7398 - val_accuracy: 0.7000 - val_loss: 0.5697\n",
      "Epoch 63/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7333 - loss: 0.7282 - val_accuracy: 0.7333 - val_loss: 0.5667\n",
      "Epoch 64/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7000 - loss: 0.7500 - val_accuracy: 0.7333 - val_loss: 0.5629\n",
      "Epoch 65/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.7015 - val_accuracy: 0.7667 - val_loss: 0.5593\n",
      "Epoch 66/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6889 - loss: 0.7154 - val_accuracy: 0.7667 - val_loss: 0.5572\n",
      "Epoch 67/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7444 - loss: 0.7439 - val_accuracy: 0.7667 - val_loss: 0.5539\n",
      "Epoch 68/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7444 - loss: 0.7027 - val_accuracy: 0.7667 - val_loss: 0.5541\n",
      "Epoch 69/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7111 - loss: 0.7012 - val_accuracy: 0.7667 - val_loss: 0.5519\n",
      "Epoch 70/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7778 - loss: 0.6789 - val_accuracy: 0.7667 - val_loss: 0.5488\n",
      "Epoch 71/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7667 - loss: 0.6737 - val_accuracy: 0.7667 - val_loss: 0.5456\n",
      "Epoch 72/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7667 - loss: 0.7120 - val_accuracy: 0.7667 - val_loss: 0.5434\n",
      "Epoch 73/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7222 - loss: 0.6667 - val_accuracy: 0.7667 - val_loss: 0.5394\n",
      "Epoch 74/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7444 - loss: 0.6959 - val_accuracy: 0.7667 - val_loss: 0.5357\n",
      "Epoch 75/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7444 - loss: 0.6630 - val_accuracy: 0.7667 - val_loss: 0.5346\n",
      "Epoch 76/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6778 - loss: 0.7372 - val_accuracy: 0.7667 - val_loss: 0.5335\n",
      "Epoch 77/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 0.7233 - val_accuracy: 0.7667 - val_loss: 0.5307\n",
      "Epoch 78/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7667 - loss: 0.6818 - val_accuracy: 0.7667 - val_loss: 0.5290\n",
      "Epoch 79/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7222 - loss: 0.7119 - val_accuracy: 0.7667 - val_loss: 0.5270\n",
      "Epoch 80/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7889 - loss: 0.6675 - val_accuracy: 0.7667 - val_loss: 0.5241\n",
      "Epoch 81/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7778 - loss: 0.6888 - val_accuracy: 0.7667 - val_loss: 0.5205\n",
      "Epoch 82/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7889 - loss: 0.6741 - val_accuracy: 0.8000 - val_loss: 0.5156\n",
      "Epoch 83/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7444 - loss: 0.6606 - val_accuracy: 0.8000 - val_loss: 0.5145\n",
      "Epoch 84/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7778 - loss: 0.6451 - val_accuracy: 0.8000 - val_loss: 0.5112\n",
      "Epoch 85/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7556 - loss: 0.6724 - val_accuracy: 0.8000 - val_loss: 0.5085\n",
      "Epoch 86/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7778 - loss: 0.6655 - val_accuracy: 0.8000 - val_loss: 0.5076\n",
      "Epoch 87/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7889 - loss: 0.6366 - val_accuracy: 0.8000 - val_loss: 0.5059\n",
      "Epoch 88/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7222 - loss: 0.6431 - val_accuracy: 0.8000 - val_loss: 0.5032\n",
      "Epoch 89/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7556 - loss: 0.6830 - val_accuracy: 0.8333 - val_loss: 0.4990\n",
      "Epoch 90/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7778 - loss: 0.6664 - val_accuracy: 0.8333 - val_loss: 0.4964\n",
      "Epoch 91/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7222 - loss: 0.6833 - val_accuracy: 0.8333 - val_loss: 0.4939\n",
      "Epoch 92/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7667 - loss: 0.6510 - val_accuracy: 0.8333 - val_loss: 0.4912\n",
      "Epoch 93/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7778 - loss: 0.5969 - val_accuracy: 0.8333 - val_loss: 0.4892\n",
      "Epoch 94/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8222 - loss: 0.5991 - val_accuracy: 0.9000 - val_loss: 0.4832\n",
      "Epoch 95/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7444 - loss: 0.6615 - val_accuracy: 0.9000 - val_loss: 0.4803\n",
      "Epoch 96/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7222 - loss: 0.6763 - val_accuracy: 0.9000 - val_loss: 0.4770\n",
      "Epoch 97/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7111 - loss: 0.6685 - val_accuracy: 0.9000 - val_loss: 0.4731\n",
      "Epoch 98/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7778 - loss: 0.6098 - val_accuracy: 0.9000 - val_loss: 0.4699\n",
      "Epoch 99/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7889 - loss: 0.6509 - val_accuracy: 0.9000 - val_loss: 0.4692\n",
      "Epoch 100/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7667 - loss: 0.6472 - val_accuracy: 0.9000 - val_loss: 0.4677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f0820612290>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=8, \n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558216c",
   "metadata": {},
   "source": [
    "## Visualization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9cd51e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" viewBox=\"607.3 116.41 1389.39 707.79\"><g transform=\"translate(-1007.4972135770904,-583.1662095830321) scale(2.0804733218772564)\"><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,346.5 991.1666666666666,346.5\" style=\"stroke-width: 0.192888; stroke-opacity: 1; stroke: rgb(255, 211, 211); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,386.5 991.1666666666666,386.5\" style=\"stroke-width: 0.51222; stroke-opacity: 1; stroke: rgb(139, 139, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,426.5 991.1666666666666,426.5\" style=\"stroke-width: 0.57349; stroke-opacity: 1; stroke: rgb(255, 126, 126); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,466.5 991.1666666666666,466.5\" style=\"stroke-width: 0.282773; stroke-opacity: 1; stroke: rgb(191, 191, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,506.5 991.1666666666666,506.5\" style=\"stroke-width: 1.12302; stroke-opacity: 1; stroke: rgb(2, 2, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,546.5 991.1666666666666,546.5\" style=\"stroke-width: 0.163581; stroke-opacity: 1; stroke: rgb(218, 218, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,586.5 991.1666666666666,586.5\" style=\"stroke-width: 0.0955988; stroke-opacity: 1; stroke: rgb(255, 233, 233); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,426.5C901.1666666666666,426.5 901.1666666666666,626.5 991.1666666666666,626.5\" style=\"stroke-width: 0.235406; stroke-opacity: 1; stroke: rgb(255, 202, 202); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,346.5 991.1666666666666,346.5\" style=\"stroke-width: 0.808817; stroke-opacity: 1; stroke: rgb(255, 72, 72); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,386.5 991.1666666666666,386.5\" style=\"stroke-width: 0.434726; stroke-opacity: 1; stroke: rgb(255, 157, 157); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,426.5 991.1666666666666,426.5\" style=\"stroke-width: 0.457626; stroke-opacity: 1; stroke: rgb(152, 152, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,466.5 991.1666666666666,466.5\" style=\"stroke-width: 0.772659; stroke-opacity: 1; stroke: rgb(81, 81, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,506.5 991.1666666666666,506.5\" style=\"stroke-width: 0.774415; stroke-opacity: 1; stroke: rgb(255, 80, 80); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,546.5 991.1666666666666,546.5\" style=\"stroke-width: 0.788226; stroke-opacity: 1; stroke: rgb(255, 77, 77); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,586.5 991.1666666666666,586.5\" style=\"stroke-width: 0.503252; stroke-opacity: 1; stroke: rgb(141, 141, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,466.5C901.1666666666666,466.5 901.1666666666666,626.5 991.1666666666666,626.5\" style=\"stroke-width: 0.905803; stroke-opacity: 1; stroke: rgb(255, 51, 51); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,346.5 991.1666666666666,346.5\" style=\"stroke-width: 0.026048; stroke-opacity: 1; stroke: rgb(255, 249, 249); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,386.5 991.1666666666666,386.5\" style=\"stroke-width: 0.236074; stroke-opacity: 1; stroke: rgb(255, 202, 202); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,426.5 991.1666666666666,426.5\" style=\"stroke-width: 0.727337; stroke-opacity: 1; stroke: rgb(255, 91, 91); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,466.5 991.1666666666666,466.5\" style=\"stroke-width: 0.968121; stroke-opacity: 1; stroke: rgb(37, 37, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,506.5 991.1666666666666,506.5\" style=\"stroke-width: 0.923294; stroke-opacity: 1; stroke: rgb(255, 47, 47); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,546.5 991.1666666666666,546.5\" style=\"stroke-width: 1.12267; stroke-opacity: 1; stroke: rgb(2, 2, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,586.5 991.1666666666666,586.5\" style=\"stroke-width: 0.57231; stroke-opacity: 1; stroke: rgb(126, 126, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,506.5C901.1666666666666,506.5 901.1666666666666,626.5 991.1666666666666,626.5\" style=\"stroke-width: 0.402583; stroke-opacity: 1; stroke: rgb(255, 164, 164); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,346.5 991.1666666666666,346.5\" style=\"stroke-width: 0.0664607; stroke-opacity: 1; stroke: rgb(255, 240, 240); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,386.5 991.1666666666666,386.5\" style=\"stroke-width: 0.440643; stroke-opacity: 1; stroke: rgb(156, 156, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,426.5 991.1666666666666,426.5\" style=\"stroke-width: 0.169716; stroke-opacity: 1; stroke: rgb(217, 217, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,466.5 991.1666666666666,466.5\" style=\"stroke-width: 0.62659; stroke-opacity: 1; stroke: rgb(114, 114, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,506.5 991.1666666666666,506.5\" style=\"stroke-width: 1.0636; stroke-opacity: 1; stroke: rgb(15, 15, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,546.5 991.1666666666666,546.5\" style=\"stroke-width: 0.0389166; stroke-opacity: 1; stroke: rgb(255, 246, 246); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,586.5 991.1666666666666,586.5\" style=\"stroke-width: 0.311592; stroke-opacity: 1; stroke: rgb(255, 185, 185); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M811.1666666666666,546.5C901.1666666666666,546.5 901.1666666666666,626.5 991.1666666666666,626.5\" style=\"stroke-width: 0.417667; stroke-opacity: 1; stroke: rgb(255, 161, 161); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,346.5C1081.1666666666667,346.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.911955; stroke-opacity: 1; stroke: rgb(255, 49, 49); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,346.5C1081.1666666666667,346.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.756617; stroke-opacity: 1; stroke: rgb(84, 84, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,346.5C1081.1666666666667,346.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.0986793; stroke-opacity: 1; stroke: rgb(233, 233, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,346.5C1081.1666666666667,346.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 1.12965; stroke-opacity: 1; stroke: rgb(0, 0, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,386.5C1081.1666666666667,386.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.859645; stroke-opacity: 1; stroke: rgb(255, 61, 61); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,386.5C1081.1666666666667,386.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.123709; stroke-opacity: 1; stroke: rgb(227, 227, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,386.5C1081.1666666666667,386.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.554044; stroke-opacity: 1; stroke: rgb(255, 130, 130); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,386.5C1081.1666666666667,386.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.279059; stroke-opacity: 1; stroke: rgb(255, 192, 192); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,426.5C1081.1666666666667,426.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.570323; stroke-opacity: 1; stroke: rgb(255, 126, 126); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,426.5C1081.1666666666667,426.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.5692; stroke-opacity: 1; stroke: rgb(127, 127, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,426.5C1081.1666666666667,426.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.159977; stroke-opacity: 1; stroke: rgb(219, 219, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,426.5C1081.1666666666667,426.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.533926; stroke-opacity: 1; stroke: rgb(135, 135, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,466.5C1081.1666666666667,466.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.0148279; stroke-opacity: 1; stroke: rgb(252, 252, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,466.5C1081.1666666666667,466.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.131469; stroke-opacity: 1; stroke: rgb(255, 225, 225); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,466.5C1081.1666666666667,466.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.912405; stroke-opacity: 1; stroke: rgb(255, 49, 49); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,466.5C1081.1666666666667,466.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.661826; stroke-opacity: 1; stroke: rgb(106, 106, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,506.5C1081.1666666666667,506.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.0526442; stroke-opacity: 1; stroke: rgb(243, 243, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,506.5C1081.1666666666667,506.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.91156; stroke-opacity: 1; stroke: rgb(255, 49, 49); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,506.5C1081.1666666666667,506.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.145969; stroke-opacity: 1; stroke: rgb(255, 222, 222); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,506.5C1081.1666666666667,506.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.958973; stroke-opacity: 1; stroke: rgb(39, 39, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,546.5C1081.1666666666667,546.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 1.1039; stroke-opacity: 1; stroke: rgb(6, 6, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,546.5C1081.1666666666667,546.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.67281; stroke-opacity: 1; stroke: rgb(103, 103, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,546.5C1081.1666666666667,546.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.485855; stroke-opacity: 1; stroke: rgb(255, 145, 145); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,546.5C1081.1666666666667,546.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.105473; stroke-opacity: 1; stroke: rgb(255, 231, 231); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,586.5C1081.1666666666667,586.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.309383; stroke-opacity: 1; stroke: rgb(255, 185, 185); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,586.5C1081.1666666666667,586.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.458835; stroke-opacity: 1; stroke: rgb(151, 151, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,586.5C1081.1666666666667,586.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 0.846178; stroke-opacity: 1; stroke: rgb(64, 64, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,586.5C1081.1666666666667,586.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.480888; stroke-opacity: 1; stroke: rgb(146, 146, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,626.5C1081.1666666666667,626.5 1081.1666666666667,426.5 1171.1666666666667,426.5\" style=\"stroke-width: 0.305641; stroke-opacity: 1; stroke: rgb(255, 186, 186); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,626.5C1081.1666666666667,626.5 1081.1666666666667,466.5 1171.1666666666667,466.5\" style=\"stroke-width: 0.216376; stroke-opacity: 1; stroke: rgb(255, 206, 206); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,626.5C1081.1666666666667,626.5 1081.1666666666667,506.5 1171.1666666666667,506.5\" style=\"stroke-width: 1.10027; stroke-opacity: 1; stroke: rgb(7, 7, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M991.1666666666666,626.5C1081.1666666666667,626.5 1081.1666666666667,546.5 1171.1666666666667,546.5\" style=\"stroke-width: 0.320394; stroke-opacity: 1; stroke: rgb(183, 183, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,426.5C1261.1666666666667,426.5 1261.1666666666667,446.5 1351.1666666666667,446.5\" style=\"stroke-width: 0.0700026; stroke-opacity: 1; stroke: rgb(255, 239, 239); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,466.5C1261.1666666666667,466.5 1261.1666666666667,446.5 1351.1666666666667,446.5\" style=\"stroke-width: 0.773541; stroke-opacity: 1; stroke: rgb(80, 80, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,506.5C1261.1666666666667,506.5 1261.1666666666667,446.5 1351.1666666666667,446.5\" style=\"stroke-width: 0.761034; stroke-opacity: 1; stroke: rgb(255, 83, 83); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,546.5C1261.1666666666667,546.5 1261.1666666666667,446.5 1351.1666666666667,446.5\" style=\"stroke-width: 0.246462; stroke-opacity: 1; stroke: rgb(255, 199, 199); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,426.5C1261.1666666666667,426.5 1261.1666666666667,486.5 1351.1666666666667,486.5\" style=\"stroke-width: 1.08905; stroke-opacity: 1; stroke: rgb(255, 9, 9); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,426.5C1261.1666666666667,426.5 1261.1666666666667,526.5 1351.1666666666667,526.5\" style=\"stroke-width: 1.03642; stroke-opacity: 1; stroke: rgb(255, 21, 21); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,466.5C1261.1666666666667,466.5 1261.1666666666667,486.5 1351.1666666666667,486.5\" style=\"stroke-width: 0.343891; stroke-opacity: 1; stroke: rgb(255, 177, 177); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,466.5C1261.1666666666667,466.5 1261.1666666666667,526.5 1351.1666666666667,526.5\" style=\"stroke-width: 0.382663; stroke-opacity: 1; stroke: rgb(169, 169, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,506.5C1261.1666666666667,506.5 1261.1666666666667,486.5 1351.1666666666667,486.5\" style=\"stroke-width: 0.197781; stroke-opacity: 1; stroke: rgb(255, 210, 210); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,506.5C1261.1666666666667,506.5 1261.1666666666667,526.5 1351.1666666666667,526.5\" style=\"stroke-width: 0.137311; stroke-opacity: 1; stroke: rgb(224, 224, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,546.5C1261.1666666666667,546.5 1261.1666666666667,486.5 1351.1666666666667,486.5\" style=\"stroke-width: 0.749168; stroke-opacity: 1; stroke: rgb(86, 86, 255); fill: none;\"/><path class=\"link\" marker-end=\"\" d=\"M1171.1666666666667,546.5C1261.1666666666667,546.5 1261.1666666666667,526.5 1351.1666666666667,526.5\" style=\"stroke-width: 0.303535; stroke-opacity: 1; stroke: rgb(187, 187, 255); fill: none;\"/><circle r=\"10\" class=\"node\" id=\"0_0\" cx=\"811.1666666666666\" cy=\"426.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"0_1\" cx=\"811.1666666666666\" cy=\"466.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"0_2\" cx=\"811.1666666666666\" cy=\"506.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"0_3\" cx=\"811.1666666666666\" cy=\"546.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_0\" cx=\"991.1666666666666\" cy=\"346.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_1\" cx=\"991.1666666666666\" cy=\"386.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_2\" cx=\"991.1666666666666\" cy=\"426.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_3\" cx=\"991.1666666666666\" cy=\"466.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_4\" cx=\"991.1666666666666\" cy=\"506.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_5\" cx=\"991.1666666666666\" cy=\"546.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_6\" cx=\"991.1666666666666\" cy=\"586.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"1_7\" cx=\"991.1666666666666\" cy=\"626.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"2_0\" cx=\"1171.1666666666667\" cy=\"426.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"2_1\" cx=\"1171.1666666666667\" cy=\"466.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"2_2\" cx=\"1171.1666666666667\" cy=\"506.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"2_3\" cx=\"1171.1666666666667\" cy=\"546.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"3_0\" cx=\"1351.1666666666667\" cy=\"446.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><text class=\"text\" dy=\".35em\" x=\"776.1666666666666\" y=\"666.5\" style=\"font-size: 12px;\">Input Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" x=\"956.1666666666666\" y=\"666.5\" style=\"font-size: 12px;\">Hidden Layer ∈ ℝ⁸</text><text class=\"text\" dy=\".35em\" x=\"1136.1666666666667\" y=\"666.5\" style=\"font-size: 12px;\">Hidden Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" x=\"1316.1666666666667\" y=\"666.5\" style=\"font-size: 12px;\">Output Layer ∈ ℝ³</text><circle r=\"10\" class=\"node\" id=\"3_1\" cx=\"1351.1666666666667\" cy=\"486.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/><circle r=\"10\" class=\"node\" id=\"3_2\" cx=\"1351.1666666666667\" cy=\"526.5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\"/></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"/></marker></defs></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "# Replace 'nn.svg' with the full path if it's in a different folder\n",
    "display(SVG(filename='NN.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3b9bc",
   "metadata": {},
   "source": [
    "# ACCURACY AND CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a06a60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f0820347a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(x_test),axis=1)\n",
    "y_predval = np.argmax(model.predict(x_val),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e506edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9666666666666667\n",
      "Validation Accuracy =  0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "accval = accuracy_score(y_val, y_predval)  # Fixed: use y_val instead of y_test\n",
    "\n",
    "print(\"Test Accuracy = \", acc)\n",
    "print(\"Validation Accuracy = \", accval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ccf69",
   "metadata": {},
   "source": [
    "### Explanation of the Learning Process\n",
    "\n",
    "During **forward propagation**, the input data passes through the input layer and then through the hidden layers, where each neuron computes a weighted sum of its inputs, applies an activation function (like ReLU), and passes the result to the next layer until the output is produced.\n",
    "\n",
    "The **error** is computed by comparing the predicted output with the true target values using a loss function such as cross-entropy or mean squared error. This loss measures how far the model’s predictions are from the actual labels.\n",
    "\n",
    "During **backpropagation**, the network calculates the gradient of the loss with respect to each weight by applying the chain rule, starting from the output layer and moving backward through the hidden layers. This tells the network how much each weight contributed to the error.\n",
    "\n",
    "The **weights are updated** using an optimizer like SGD or Adam, which adjusts them slightly in the direction that reduces the loss. Over many training epochs, these updates gradually minimize the error and improve the model’s performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
